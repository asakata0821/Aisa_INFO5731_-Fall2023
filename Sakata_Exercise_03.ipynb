{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asakata0821/Aisa_INFO5731_-Fall2023/blob/main/Sakata_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "I chose the Summarization system to answer this question because if we can summarize documents, it can save us a lot of time.\n",
        "Also, I'm working on Japanese-American Incarceration history research now. I  want to apply this system to the research as well for more efficient work.\n",
        "Note: I wrote codes for just extracting features for this assignmen, outputs are not accurate.\n",
        "      I assume these codes will be fixed to give the right outputs for the future work.\n",
        "\n",
        "\n",
        "There are 5 features which can be helpful for making the system better.\n",
        "-TF-IDF\n",
        "-Named Entity Recognition (NER) Tags\n",
        "-Sentence Position\n",
        "-Sentence Length\n",
        "-Readability Scores\n",
        "\n",
        "Reasons why they can be helpful are below:\n",
        "TF-IDF: Based on the frequency of words, it assigns scores indicating the importance of each word.\n",
        "        This helps us list those words and summarize the document according to their scores.\n",
        "\n",
        "NER Tags: It identifies named entities. These entities often have higher scores of importance in TF-IDF.\n",
        "\n",
        "Sentence Length: Shorter sentences tend to be useful for summarization, as they often capture key information.\n",
        "\n",
        "Keyword Density: Based on the TF-IDF scores, I define the top 10 words as keywords for the passage (document).\n",
        "                 Then calculate the keyword density to analyze the most important or prominent terms in the text.\n",
        "\n",
        "Readability Score: Structurally simple and less complex sentences are better for summarization, which may enhance the overall readability of the summary.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ARA_V6FslA04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For Feature 1\n",
        "def calculate_tfidf_scores(texts):\n",
        "    tfidf = TfidfVectorizer()\n",
        "    features = tfidf.fit_transform(texts)\n",
        "    word = tfidf.get_feature_names_out()\n",
        "    tfidf_scores = features.toarray()[0]\n",
        "\n",
        "    return word, tfidf_scores\n",
        "\n",
        "#For Feature 2\n",
        "def ner_tags(texts):\n",
        "    doc = nlp(texts)\n",
        "    ner_tags = [ent.label_ for ent in doc.ents]\n",
        "    return ', '.join(ner_tags)\n",
        "\n",
        "#For Feature 3\n",
        "def Sentence_Length(texts):\n",
        "    sentences = []\n",
        "    word_counts = []\n",
        "\n",
        "    def count_words(sentence):\n",
        "        words = sentence.split()\n",
        "        return len(words)\n",
        "\n",
        "    #splitting a whole texts into sentences and storing\n",
        "    for idx, sentence in enumerate(texts, start=1):\n",
        "        sentence_parts = sentence.split('.')\n",
        "\n",
        "        for part in sentence_parts:\n",
        "            # Counting words\n",
        "            word_count = count_words(part.strip())\n",
        "            sentences.append(part.strip())\n",
        "            word_counts.append(word_count)\n",
        "\n",
        "    return sentences, word_counts\n",
        "\n",
        "\n",
        "#For Feature 4\n",
        "def keyword(tfidf_scores, word):\n",
        "    #extracting the top 10 words and calculate TDTF-IDF scores for each\n",
        "    top10_indices = tfidf_scores.argsort()[-10:][::-1]\n",
        "    top10_scores = tfidf_scores[top10_indices]\n",
        "    top10_words = [word[idx] for idx in top10_indices]\n",
        "\n",
        "    return top10_words, top10_scores\n",
        "\n",
        "def calculate_keyword_density(texts, keywords):\n",
        "    densities = []\n",
        "\n",
        "    for text in texts:\n",
        "        words = text.lower().split()\n",
        "\n",
        "        # For the case when there are no words\n",
        "        if not words:\n",
        "            densities.append(0)\n",
        "            continue\n",
        "\n",
        "        total_words = len(words)\n",
        "        keyword_counter = 0\n",
        "\n",
        "        #count the keyword numbers in sentences\n",
        "        for keyword in keywords:\n",
        "            keyword_counter += sum(1 for word in words if word == keyword)\n",
        "\n",
        "        # Calculate the average of keyword density(%)\n",
        "        keyword_density = (keyword_counter / total_words) * 100\n",
        "        densities.append(keyword_density)\n",
        "\n",
        "    return densities\n",
        "\n",
        "#For Feature 5\n",
        "def Readability_Scores(avg_words_per_sentence, avg_syllables_per_word):\n",
        "    return 206.835 - 1.015 * avg_words_per_sentence - 84.6 * avg_syllables_per_word\n",
        "\n",
        "def count_syllables(word):\n",
        "    # Count syllable\n",
        "    syllables = re.findall(r'[aeiouyAEIOUY]+', word)\n",
        "    # Exclude parts where has 'e' in the end of words\n",
        "    syllables = [syl for syl in syllables if not syl.endswith('e')]\n",
        "    return len(syllables)\n",
        "\n",
        "def calculate_avg_syllables_per_word(text):\n",
        "    words = text.split()\n",
        "    total_syllables = sum(count_syllables(word) for word in words if word)\n",
        "    total_words = len(words)\n",
        "    if total_words == 0:\n",
        "        return 0\n",
        "    return total_syllables / total_words\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pGbd1A6E0UMh"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "423a9b86-0afa-4601-8aa0-359919632494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Word       Feature 1: TF-IDF Scores Feature 2: NER Tags\n",
            "0    2009      0.000000                  DATE              \n",
            "1    27        0.000000                  CARDINAL          \n",
            "2    above     0.074953                                    \n",
            "3    ad        0.000000                                    \n",
            "4    after     0.000000                                    \n",
            "..     ...          ...                 ..                 \n",
            "204  writings  0.000000                                    \n",
            "205  wrote     0.074953                                    \n",
            "206  wwii      0.074953                  PERSON            \n",
            "207  years     0.095684                  DATE              \n",
            "208  young     0.059094                                    \n",
            "\n",
            "[209 rows x 3 columns]\n",
            "   Sentence                                                                                                                                                                                                                                                                                   Feature 3: Sentence Length  Feature 4: Keyword Density\n",
            "1   This story of Aiko Ebihara really begins in the friendship forged between two families living in Salem, Oregon prior to the start of WWII                                                                                                                                                 24                          33.333333                 \n",
            "2   Aiko’s parents, Maki and Frank, were restaurant owners and full-time cooks at Tokio Sukiyaki, living above the restaurant in a cramped bedroom with three young children                                                                                                                  26                          19.230769                 \n",
            "3   With Aiko on the way, there was simply no room for a newborn baby and an arrangement was made for a family in town, the Williams, to care for her for a few years                                                                                                                         34                          41.176471                 \n",
            "4   The arrangement worked out so well that Velora, the only child of Edward and Alice Williams, would come to embrace Aiko as her own                                                                                                                                                        24                          29.166667                 \n",
            "5   “I was so proud of Aiko! I wanted to claim her as my little sister,” she wrote                                                                                                                                                                                                            17                          17.647059                 \n",
            "6   For three years, the Williams raised Aiko in their home, with frequent visits and check-ins with the Tanakas                                                                                                                                                                              18                          44.444444                 \n",
            "7                                                                                                                                                                                                                                                                                             0                           0.000000                  \n",
            "8   But when the war broke out, anti-Japanese sentiment quietly but swiftly permeated the town                                                                                                                                                                                                14                          14.285714                 \n",
            "9   The Tanaka’s business took a nosedive, despite the efforts of Frank Tanaka to prove his patriotism: He took out an ad in the local paper pleading, “I have been a resident of Salem for over 27 years                                                                                     37                          21.621622                 \n",
            "10  I love my wife, I love my children, I love my home and I love my United States                                                                                                                                                                                                            18                          5.555556                  \n",
            "11  ” The Williams seemed to be the only family that stood by them without question                                                                                                                                                                                                           15                          20.000000                 \n",
            "12                                                                                                                                                                                                                                                                                            0                           0.000000                  \n",
            "13  After Velora passed in 2009, Aiko received boxes of letters, photos, and journal entries kept meticulously by her old friend, but with very little organization                                                                                                                           25                          20.000000                 \n",
            "14  Picking up the mantle, Aiko worked with an Oberlin College professor to rework Velora’s precious collection into a book titled Aiko: The Story of a Young, Japanese-American Woman, which is available at the Hirasaki National Resource Center at the Japanese American National Museum  43                          18.604651                 \n",
            "15                                                                                                                                                                                                                                                                                            0                           0.000000                  \n",
            "16  Aiko’s spotty memories of camp and her early years in Salem were preserved through the eyes of Velora, whose compassion and love for her permeate the writings found in Aiko                                                                                                              30                          33.333333                 \n",
            "17  From visiting the Tanakas in Tule Lake to remaining lifelong pen pals, it was this kind of unconditional respect — and the Williams’ refusal to succumb to fear-mongering and prejudice — that would define the long-standing friendship between the two families                         41                          29.268293                 \n",
            "18  Mr                                                                                                                                                                                                                                                                                        1                           0.000000                  \n",
            "19  Williams even carried a photo of Aiko with him until the day he died                                                                                                                                                                                                                      14                          28.571429                 \n",
            "20                                                                                                                                                                                                                                                                                            0                           0.000000                  \n",
            "************************************************************\n",
            "Feature 5: Readability Score 88.22810319767443\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Load spaCy model for named entity recognition\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#Collecting information by web scraping\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "}\n",
        "\n",
        "url = 'https://www.jamsj.org/manabu/aiko-ebihara'\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "elements = soup.find_all(class_='sqs-block html-block sqs-block-html', id='block-yui_3_17_2_1_1587148327627_5945')\n",
        "\n",
        "texts = []\n",
        "\n",
        "for element in elements:\n",
        "    paragraphs = element.find_all('p')\n",
        "    for p in paragraphs:\n",
        "        text = p.get_text()\n",
        "        texts.append(text)\n",
        "\n",
        "\n",
        "\n",
        "#1 TF-IDF\n",
        "word, tfidf_scores = calculate_tfidf_scores(texts)\n",
        "df = pd.DataFrame({\n",
        "        'Word': word,\n",
        "        'Feature 1: TF-IDF Scores': tfidf_scores,\n",
        "    })\n",
        "\n",
        "\n",
        "#2 NER tag\n",
        "df['Feature 2: NER Tags'] = df['Word'].apply(ner_tags)\n",
        "print(df)\n",
        "\n",
        "\n",
        "#3 sentence length\n",
        "sentences, word_counts = Sentence_Length(texts)\n",
        "\n",
        "\n",
        "#4 keyword density\n",
        "#for focusing more frequent words, extactthe top 10 words and define them as keywords\n",
        "top10_words, top10_scores = keyword(tfidf_scores, word)\n",
        "\n",
        "sentences = df2['Sentence']\n",
        "keywords = top10_words\n",
        "\n",
        "density = calculate_keyword_density(sentences, keywords)\n",
        "\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    'Sentence': sentences,\n",
        "    'Feature 3: Sentence Length': word_counts,\n",
        "    'Feature 4: Keyword Density':density\n",
        "}, index=range(1, len(sentences) + 1))\n",
        "\n",
        "\n",
        "print(df2)\n",
        "\n",
        "#5 readablity scores\n",
        "words_per_sentence = df2['Feature 3: Sentence Length']\n",
        "sum_of_words = sum(words_per_sentence)\n",
        "\n",
        "#Removing sentences which has no characters (blank lines)\n",
        "valid_sentences = df2[df2['Feature 3: Sentence Length'] != 0]\n",
        "num_of_sentences = len(valid_sentences)\n",
        "\n",
        "avg_words_per_sentence = sum_of_words / num_of_sentences\n",
        "\n",
        "avg_syllables = calculate_avg_syllables_per_word(text)\n",
        "\n",
        "fres_score = Readability_Scores(avg_words_per_sentence, avg_syllables)\n",
        "\n",
        "print('************************************************************')\n",
        "print(\"Feature 5: Readability Score\",fres_score)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f6e8fe-6099-4085-c37a-6fbff151a2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Word      Dimension 1  Dimension 2  Dimension 3  Dimension 4  Dimension 5  Dimension 6  Dimension 7  Dimension 8  Dimension 9  ...  Dimension 87  Dimension 88  Dimension 89  Dimension 90  Dimension 91  Dimension 92  Dimension 93  Dimension 94  Dimension 95  Dimension 96\n",
            "0    This     0.196376    -0.225602    -0.216389     1.606050     0.116756    -0.023740    -0.677747     0.338733    -0.814684     ...  1.175289      0.024106      0.718536     -0.875431     -0.846864     -0.211391      0.872998     -0.635098     -1.227015     -0.484034    \n",
            "1    story   -0.626435     0.170139    -1.241805    -0.829305     0.762212    -0.108243     2.029763     1.126165    -0.431281     ...  0.072290      0.141888     -1.006470     -0.326446     -0.420018     -0.466750      1.227321      0.568568     -0.822245      1.637805    \n",
            "2    of       1.597100    -0.232040     0.284133    -0.391191    -0.869625    -0.926237    -0.092524    -0.670386     0.764744     ... -1.318537      1.599347     -0.605079     -0.859557      0.113933     -0.126686     -1.120528      0.168356     -0.016124      0.530831    \n",
            "3    Aiko     0.115260    -0.095934     0.107825     1.541098     0.202363     0.891307     0.486904     1.509227     0.361727     ... -0.410498     -0.135529     -1.288999      0.946692      0.321324     -0.280375      2.726099      0.431945     -0.071622     -0.198192    \n",
            "4    Ebihara -1.361111    -2.131041    -0.929870     0.672527     1.693646    -0.563327     1.067352    -0.051135    -0.324800     ...  1.089341     -0.060648      0.005179     -0.251382     -0.186706     -0.681187      2.959441     -0.952766      0.063002      0.308988    \n",
            "..       ...       ...          ...          ...          ...          ...          ...          ...          ...          ...     ...       ...           ...           ...           ...           ...           ...           ...           ...           ...           ...    \n",
            "441  the      2.453153     0.742245     0.634697     0.275072     0.127726    -0.049971     0.368171    -0.095876    -0.652560     ...  1.206995     -1.231908     -0.797929     -0.040856      1.147452      0.321533      0.264157     -0.779549     -0.563089     -1.491860    \n",
            "442  day     -0.896412    -0.853653    -1.097235    -1.306600     0.685029    -0.773536     0.421344     1.082297    -1.180544     ...  0.829602     -0.019162     -0.298387      0.334238     -0.834528     -1.013232      0.609865      1.106653      0.016300      0.878618    \n",
            "443  he      -0.874676    -1.026573     0.847075    -0.279601    -0.968914    -0.193124     2.082947     0.030037     0.327687     ...  0.590285      0.224679      0.456895     -0.038089      1.170986      0.073354      0.414748      0.515677      0.092880     -0.114495    \n",
            "444  died     1.139403     0.843619    -0.838760    -0.267288    -0.281640    -0.884142    -1.240990    -0.027258    -0.018491     ...  1.121352     -0.532498     -1.396718      0.296077      0.543220      0.331986     -0.545780      0.785671      2.110494      1.127227    \n",
            "445  .       -1.010701    -0.227509     0.330966    -1.757784    -0.208362     1.942571     0.812422    -0.398828    -0.121607     ...  0.023489      1.230921     -1.964421      0.279221     -1.154695      0.264305     -0.188054     -0.204830     -0.696127     -0.998397    \n",
            "\n",
            "[446 rows x 97 columns]\n"
          ]
        }
      ],
      "source": [
        "#I chose Word Embedding. This helps us to define classifications or topic categories for documents.\n",
        "\n",
        "def Word_Embeddings(texts):\n",
        "    words = []\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        for i in doc:\n",
        "            words.append(i.text)\n",
        "            embeddings.append(i.vector)\n",
        "    return words, np.array(embeddings)\n",
        "\n",
        "words, embeddings = Word_Embeddings(texts)\n",
        "\n",
        "df3 = pd.DataFrame(embeddings, columns=[f'Dimension {i+1}' for i in range(embeddings.shape[1])])\n",
        "df3.insert(0, 'Word', words)\n",
        "\n",
        "print(df3)\n",
        "\n",
        "\n",
        "\n",
        "#Importance Ranking\n",
        "#1. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "#2. Named Entity Recognition (NER) Tags\n",
        "#3. Word Embedding\n",
        "#4. Sentence Length\n",
        "#5. Readability Scores\n",
        "#6. Sentence Position\n",
        "\n",
        "#I made this ranking as considering the usability for Oral History Digital Achiving.\n",
        "#TF-IDF came first because it can identify unique and significant terms in documents.\n",
        "#Named Entity Recognition (NER) Tags is useful for capturing named entities which are very crucial for especially historical data.\n",
        "#Word Embedding is more abstract representation compared to the top 2 methods.\n",
        "#Sentence Length, Readability Scores, and Sentence Position are also significant but slightly less informative for capturing content essence in oral historical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cocvL31IGUH1",
        "outputId": "b7824db5-fbe0-42ee-c96e-9b8c16c13465"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b8021b-ec54-493c-b416-93c95b0db39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: Similarity = 0.3772, Sentence: Mr\n",
            "2: Similarity = 0.2966, Sentence: For three years, the Williams raised Aiko in their home, with frequent visits and check-ins with the Tanakas\n",
            "3: Similarity = 0.2938, Sentence: Williams even carried a photo of Aiko with him until the day he died\n",
            "4: Similarity = 0.2869, Sentence: But when the war broke out, anti-Japanese sentiment quietly but swiftly permeated the town\n",
            "5: Similarity = 0.2824, Sentence: With Aiko on the way, there was simply no room for a newborn baby and an arrangement was made for a family in town, the Williams, to care for her for a few years\n",
            "6: Similarity = 0.2780, Sentence: This story of Aiko Ebihara really begins in the friendship forged between two families living in Salem, Oregon prior to the start of WWII\n",
            "7: Similarity = 0.2777, Sentence: I love my wife, I love my children, I love my home and I love my United States\n",
            "8: Similarity = 0.2718, Sentence: The arrangement worked out so well that Velora, the only child of Edward and Alice Williams, would come to embrace Aiko as her own\n",
            "9: Similarity = 0.2681, Sentence: After Velora passed in 2009, Aiko received boxes of letters, photos, and journal entries kept meticulously by her old friend, but with very little organization\n",
            "10: Similarity = 0.2674, Sentence: ” The Williams seemed to be the only family that stood by them without question\n",
            "11: Similarity = 0.2541, Sentence: “I was so proud of Aiko! I wanted to claim her as my little sister,” she wrote\n",
            "12: Similarity = 0.2285, Sentence: Aiko’s parents, Maki and Frank, were restaurant owners and full-time cooks at Tokio Sukiyaki, living above the restaurant in a cramped bedroom with three young children\n",
            "13: Similarity = 0.2164, Sentence: Aiko’s spotty memories of camp and her early years in Salem were preserved through the eyes of Velora, whose compassion and love for her permeate the writings found in Aiko\n",
            "14: Similarity = 0.2075, Sentence: The Tanaka’s business took a nosedive, despite the efforts of Frank Tanaka to prove his patriotism: He took out an ad in the local paper pleading, “I have been a resident of Salem for over 27 years\n",
            "15: Similarity = 0.2022, Sentence: From visiting the Tanakas in Tule Lake to remaining lifelong pen pals, it was this kind of unconditional respect — and the Williams’ refusal to succumb to fear-mongering and prejudice — that would define the long-standing friendship between the two families\n",
            "16: Similarity = 0.1888, Sentence: Picking up the mantle, Aiko worked with an Oberlin College professor to rework Velora’s precious collection into a book titled Aiko: The Story of a Young, Japanese-American Woman, which is available at the Hirasaki National Resource Center at the Japanese American National Museum\n"
          ]
        }
      ],
      "source": [
        "#This is also I assume that I'll fix for the future work. For now, outputs are not accurate.\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# From previous df, extracting only sentences which are usable\n",
        "sentences = list(valid_sentences['Sentence'])\n",
        "query = \"place\"\n",
        "\n",
        "# Tokenize the query and sentences\n",
        "tokenized_query = tokenizer(query, return_tensors='pt')\n",
        "tokenized_sentences = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Get embeddings using BERT model\n",
        "with torch.no_grad():\n",
        "    query_embeddings = model(**tokenized_query).last_hidden_state.mean(dim=1).squeeze()\n",
        "    sentence_embeddings = model(**tokenized_sentences).last_hidden_state.mean(dim=1).squeeze()\n",
        "\n",
        "# Reshape the embeddings to align for cosine similarity calculation\n",
        "query_embeddings = query_embeddings.reshape(1, -1)\n",
        "sentence_embeddings = sentence_embeddings.reshape(len(sentences), -1)\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity = cosine_similarity(query_embeddings, sentence_embeddings).squeeze()\n",
        "\n",
        "ranking = np.argsort(similarity)[::-1]\n",
        "\n",
        "for i, rank in enumerate(ranking):\n",
        "    print(f\"{i+1}: Similarity = {similarity[rank]:.4f}, Sentence: {sentences[rank]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZcznhB_HbvV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}